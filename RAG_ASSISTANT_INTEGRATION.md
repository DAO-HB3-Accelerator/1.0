# Интеграция RAG-ассистента для бизнеса с поддержкой продуктов, сегментов клиентов и LLM

## Цель

Реализовать интеллектуального ассистента для бизнеса, который:
- Использует RAG-таблицы для хранения вопросов, ответов, уточняющих вопросов, ответов на возражения и дополнительного контекста.
- Поддерживает фильтрацию по продуктам, сегментам клиентов (тегам), приоритету, дате и другим бизнес-полям.
- Интегрируется с LLM (Ollama/OpenAI) для генерации финального ответа на основе найденного контекста.
- Позволяет настраивать системный промт с плейсхолдерами для гибкой персонализации ответов.

---

## Основные требования

1. **Структура RAG-таблицы**
   - Каждая строка содержит:
     - Вопрос (`question`)
     - Ответ (`answer`)
     - Ответ с уточняющим вопросом (`clarifyingAnswer`)
     - Ответ на возражение (`objectionAnswer`)
     - Теги пользователя/сегмента (`userTags`)
     - Продукт/услуга (`product`)
     - Дополнительный контекст (`context`)
     - Приоритет (`priority`)
     - Дата (`date`)
   - Для каждого столбца указывается назначение (purpose) через выпадающий список при создании/редактировании.

2. **Фильтрация и поиск**
   - При поступлении вопроса пользователя:
     - Фильтровать строки по продукту, тегам пользователя, приоритету, дате и другим полям.
     - Выполнять векторный поиск (embedding) только по релевантным строкам.

3. **Интеграция с LLM**
   - После поиска по RAG-таблице формировать системный промт с подстановкой найденных данных (через плейсхолдеры).
   - Передавать промт и вопрос пользователя в LLM (Ollama/OpenAI).
   - Возвращать финальный ответ пользователю.

4. **Плейсхолдеры для промта**
   - Поддерживаются плейсхолдеры:
     - `{context}` — дополнительная информация
     - `{answer}` — основной ответ
     - `{clarifyingAnswer}` — уточняющий вопрос
     - `{objectionAnswer}` — ответ на возражение
     - `{question}` — вопрос пользователя
     - `{userTags}` — теги пользователя
     - `{product}` — продукт/услуга
     - `{priority}` — приоритет
     - `{date}` — дата
     - `{rules}` — описание применённых правил
     - `{history}` — история диалога
     - `{model}` — используемая LLM
     - `{language}` — язык ответа

5. **Кэширование embedding**
   - Для ускорения поиска embedding для вопросов кэшируются в БД.
   - При изменении вопроса embedding обновляется.

6. **Логирование и аналитика**
   - Логируются все этапы работы ассистента: запрос пользователя, найденный контекст, результат LLM, время ответа, id пользователя и т.д.
   - Вся информация сохраняется для последующего анализа и улучшения качества ответов.

---

## Пример бизнес-сценария

- Клиент B2B интересуется продуктом "ProductX".
- Вопрос: "Как интегрировать ваш продукт с нашей ERP?"
- Система фильтрует строки по `product = "ProductX"` и тегу `B2B`.
- Векторный поиск проводится только по релевантным строкам.
- В системном промте используются плейсхолдеры для подстановки найденных данных.
- LLM генерирует финальный ответ с учётом контекста, уточняющих вопросов и ответов на возражения.

---

## Пример системного промта

```
Ты — ассистент компании. Пользователь интересуется продуктом: {product}, сегмент: {userTags}.
Используй только релевантные ответы и контекст для этого продукта и типа клиента.
Контекст: {context}
Ответ: {answer}
Уточняющий вопрос: {clarifyingAnswer}
Ответ на возражение: {objectionAnswer}
```

---

## Результат

- Персонализированные, точные и масштабируемые ответы для разных продуктов и сегментов клиентов.
- Гибкая настройка ассистента через UI и системный промт.
- Возможность расширения под любые бизнес-сценарии. 